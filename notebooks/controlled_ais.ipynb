{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:12:25.254144: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-11 18:12:25.254227: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-11 18:12:25.254235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dds.configs.config import set_task, get_config\n",
    "from dds.train_dds import train_dds\n",
    "import numpy as onp\n",
    "import jax\n",
    "\n",
    "import distrax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "from jaxline import utils\n",
    "\n",
    "import functools\n",
    "import timeit\n",
    "from typing import Any, List, Tuple, Optional\n",
    "from absl import app, flags\n",
    "\n",
    "from absl import logging\n",
    "import haiku as hk\n",
    "import distrax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from ml_collections import config_dict as configdict\n",
    "from ml_collections import config_flags\n",
    "\n",
    "import numpy as onp\n",
    "import optax\n",
    "\n",
    "from jaxline import utils\n",
    "\n",
    "from dds.configs.config import set_task\n",
    "from dds.data_paths import results_path\n",
    "from dds.utils import flatten_nested_dict\n",
    "import wandb\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dds.objectives import controlled_ais_relative_kl_objective\n",
    "from dds.objectives import controlled_ais_importance_weighted_partition_estimate_dds\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "# Time and step settings (Need to be done before calling set_task)\n",
    "config.model.tfinal = 6.4\n",
    "config.model.dt = 0.05\n",
    "\n",
    "config.model.reference_process_key = \"cais\"\n",
    "\n",
    "config.trainer.objective = controlled_ais_relative_kl_objective\n",
    "config.trainer.lnz_is_estimator = controlled_ais_importance_weighted_partition_estimate_dds\n",
    "\n",
    "if config.model.reference_process_key == \"oudstl\":\n",
    "    config.model.step_scheme_key = \"cos_sq\"\n",
    "\n",
    "config = set_task(config, \"funnel\")\n",
    "\n",
    "if config.model.reference_process_key == \"oudstl\":\n",
    "    config.model.step_scheme_key = \"cos_sq\"\n",
    "    \n",
    "    # Opt setting for funnel\n",
    "    config.model.sigma = 1.075\n",
    "    config.model.alpha = 0.6875\n",
    "    config.model.m = 1.0\n",
    "        \n",
    "    # Path opt settings    \n",
    "    config.model.exp_dds = False\n",
    "\n",
    "if config.model.reference_process_key == \"cais\":\n",
    "    config.model.step_scheme_key = \"cos_sq\"\n",
    "    \n",
    "    # Opt setting for funnel\n",
    "    config.model.sigma = 1.075\n",
    "    config.model.alpha = 0.6875\n",
    "    config.model.m = 1.0\n",
    "        \n",
    "    # Path opt settings    \n",
    "    config.model.exp_dds = False\n",
    "\n",
    "\n",
    "config.model.stl = False\n",
    "config.model.detach_stl_drift = False\n",
    "\n",
    "config.trainer.notebook = True\n",
    "config.trainer.epochs = 200  #  11000\n",
    "# Opt settings we use\n",
    "# funnel_config.trainer.learning_rate = 0.0001\n",
    "config.trainer.learning_rate = 5 * 10**(-3)\n",
    "config.trainer.lr_sch_base_dec = 0.95 # For funnel\n",
    "config.model.reference_process_key = \"cais\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_detached_params(trainable_params, non_trainable_params,\n",
    "                           attached_network_name=\"simple_drift_net\",\n",
    "                           detached_network_name=\"stl_detach\"):\n",
    "  \"\"\"Auxiliary function updating detached params for STL.\n",
    "\n",
    "  Args:\n",
    "      trainable_params:\n",
    "      non_trainable_params:\n",
    "      attached_network_name:\n",
    "      detached_network_name:\n",
    "  Returns:\n",
    "    Returns non trainable params\n",
    "  \"\"\"\n",
    "\n",
    "  if len(trainable_params) != len(non_trainable_params):\n",
    "    return non_trainable_params\n",
    "\n",
    "  for key in trainable_params.keys():\n",
    "    if attached_network_name in key:\n",
    "      key_det = key.replace(attached_network_name, detached_network_name)\n",
    "    else:\n",
    "      key_det = key.replace(\"diffusion_network\",\n",
    "                            detached_network_name + \"_diff\")\n",
    "    non_trainable_params[key_det] = trainable_params[key]  # pytype: disable=unsupported-operands\n",
    "\n",
    "  return non_trainable_params\n",
    "\n",
    "data_dim = config.model.input_dim\n",
    "device_no = jax.device_count()\n",
    "\n",
    "results_path = '~/denoising_diffusion_samplers/results'\n",
    "alpha = config.model.alpha\n",
    "sigma = config.model.sigma\n",
    "m = config.model.m\n",
    "\n",
    "# post setup model vars\n",
    "config.model.source_obj = distrax.MultivariateNormalDiag(\n",
    "    jnp.zeros(config.model.input_dim),\n",
    "    config.model.sigma * jnp.ones(config.model.input_dim))\n",
    "config.model.source = config.model.source_obj.log_prob\n",
    "\n",
    "batch_size_ = int(config.model.batch_size / device_no)\n",
    "batch_size_elbo = int(config.model.elbo_batch_size / device_no)\n",
    "\n",
    "step_scheme = config.model.step_scheme_dict[config.model.step_scheme_key]\n",
    "\n",
    "dt = config.model.dt\n",
    "\n",
    "if config.model.reference_process_key == \"oududp\":\n",
    "    key_conversion = {\n",
    "        \"pis\": \"pisudp\",\n",
    "        \"vanilla\": \"vanilla_udp\",\n",
    "        \"tmpis\": \"tmpis_udp\"\n",
    "    }\n",
    "    # \"pisudp\"\n",
    "    config.model.network_key = key_conversion[config.model.network_key]\n",
    "\n",
    "net_key = config.model.network_key\n",
    "network = config.model.network_dict[net_key]\n",
    "\n",
    "tpu = config.model.tpu\n",
    "\n",
    "detach_dif_path, detach_dritf_path = (\n",
    "    config.model.detach_path, config.model.detach_path)\n",
    "\n",
    "target = config.model.target\n",
    "\n",
    "tfinal = config.model.tfinal\n",
    "lnpi = config.trainer.lnpi\n",
    "\n",
    "ref_proc_key = config.model.reference_process_key\n",
    "ref_proc = config.model.reference_process_dict[ref_proc_key]\n",
    "\n",
    "trim = (2 if \"stl\" in str(ref_proc).lower() or \"udp\" in str(ref_proc).lower()\n",
    "        else 1)\n",
    "\n",
    "stl = config.model.stl\n",
    "\n",
    "brown = \"brown\" in str(ref_proc).lower()\n",
    "\n",
    "seed = config.trainer.random_seed  if \"random_seed\" in config.trainer else 42\n",
    "\n",
    "# task directory (currently not in use)\n",
    "task = config.task\n",
    "method = config.model.reference_process_key\n",
    "task_path = results_path + f\"/{task}\" + f\"/{ref_proc_key}\" + f\"/{net_key}\"\n",
    "task_path += f\"/{method}\"\n",
    "\n",
    "\n",
    "# checkpoiting variables for wandb\n",
    "nsteps = config.model.ts.shape[0]\n",
    "keep_every_nth = int(config.trainer.epochs / 125)\n",
    "file_name = (f\"/alpha_{alpha}_sigma_{sigma}_epochs_{config.trainer.epochs}\" +\n",
    "            f\"_task_{task}_seed_{seed}_steps_{nsteps}_stl_{stl}_{method}\" +\n",
    "            f\"_scheme_{config.model.step_scheme_key}_ddpm_test11_chk\")\n",
    "_ = task_path + file_name\n",
    "\n",
    "detach_stl_drift = (\n",
    "    config.model.detach_stl_drift if\n",
    "    \"detach_stl_drift\" in config.model else False\n",
    ")\n",
    "\n",
    "drift_network = lambda: network(config.model, data_dim, \"simple_drift_net\")\n",
    "\n",
    "############## wandb logging  place holder ################\n",
    "data_id = \"denoising_diffusion_samplers\"  # Project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformedWithState(init=<function transform_with_state.<locals>.init_fn at 0x7f5f61ffaf80>, apply=<function transform_with_state.<locals>.apply_fn at 0x7f5f61ffb130>)\n"
     ]
    }
   ],
   "source": [
    "from turtle import forward\n",
    "\n",
    "\n",
    "def _forward_fn(batch_size: int,\n",
    "                training: bool = True,\n",
    "                ode=False, exact=False, dt_=dt) -> jnp.ndarray:\n",
    "\n",
    "    model_def = ref_proc(\n",
    "        sigma, \n",
    "        data_dim, \n",
    "        drift_network, \n",
    "        tfinal=tfinal, \n",
    "        dt=dt_,\n",
    "        step_scheme=step_scheme, \n",
    "        alpha=alpha, \n",
    "        target=target, \n",
    "        tpu=tpu,\n",
    "        detach_stl_drift=detach_stl_drift, \n",
    "        diff_net=None,\n",
    "        detach_dritf_path=detach_dritf_path, \n",
    "        detach_dif_path=detach_dif_path,\n",
    "        m=m, \n",
    "        log=config.model.log, \n",
    "        exp_bool=config.model.exp_dds, \n",
    "        exact=exact\n",
    "    )\n",
    "\n",
    "    return model_def(batch_size, training, ode=ode)\n",
    "\n",
    "forward_fn = hk.transform_with_state(_forward_fn)  \n",
    "\n",
    "print(forward_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot concatenate arrays with different numbers of dimensions: got (300, 12), (128, 300, 12).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m subkeys \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbcast_local_devices(rng_key)\n\u001b[1;32m      7\u001b[0m p_init \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mpmap(\n\u001b[1;32m      8\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(forward_fn\u001b[38;5;241m.\u001b[39minit, batch_size\u001b[38;5;241m=\u001b[39mbatch_size_,\n\u001b[1;32m      9\u001b[0m                         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), axis_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_devices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m params, model_state \u001b[38;5;241m=\u001b[39m \u001b[43mp_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m trainable_params, non_trainable_params \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39mdata_structures\u001b[38;5;241m.\u001b[39mpartition(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m module, name, value: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstl_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m module, params)\n\u001b[1;32m     16\u001b[0m clipper \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m1.0\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/site-packages/haiku/_src/transform.py:338\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.init_fn\u001b[0;34m(rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base\u001b[38;5;241m.\u001b[39mnew_context(rng\u001b[38;5;241m=\u001b[39mrng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[1;32m    337\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [4], line 28\u001b[0m, in \u001b[0;36m_forward_fn\u001b[0;34m(batch_size, training, ode, exact, dt_)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_fn\u001b[39m(batch_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      5\u001b[0m                 training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                 ode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dt_\u001b[38;5;241m=\u001b[39mdt) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jnp\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m      8\u001b[0m     model_def \u001b[38;5;241m=\u001b[39m ref_proc(\n\u001b[1;32m      9\u001b[0m         sigma, \n\u001b[1;32m     10\u001b[0m         data_dim, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m         exact\u001b[38;5;241m=\u001b[39mexact\n\u001b[1;32m     26\u001b[0m     )\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/site-packages/haiku/_src/module.py:426\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    424\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 426\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/site-packages/haiku/_src/module.py:272\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 272\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    276\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method)\n\u001b[1;32m    277\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m/local/scratch/home/fav25/DDS/dds/stl_samplers.py:463\u001b[0m, in \u001b[0;36mAugmentedControlledAIS.__call__\u001b[0;34m(self, batch_size, is_training, dt, ode, exact)\u001b[0m\n\u001b[1;32m    461\u001b[0m key \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39mnext_rng_key()\n\u001b[1;32m    462\u001b[0m dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m is_training \u001b[38;5;28;01melse\u001b[39;00m dt\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_aug_trajectory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/site-packages/haiku/_src/module.py:426\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    424\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 426\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/site-packages/haiku/_src/module.py:272\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 272\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    276\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method)\n\u001b[1;32m    277\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m/local/scratch/home/fav25/DDS/dds/stl_samplers.py:595\u001b[0m, in \u001b[0;36mAugmentedControlledAIS.sample_aug_trajectory\u001b[0;34m(self, batch_size, key, dt, rng, **_)\u001b[0m\n\u001b[1;32m    592\u001b[0m zeros \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    593\u001b[0m y0_aug \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((y0, zeros, zeros), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 595\u001b[0m param_trajectory, ts \u001b[38;5;241m=\u001b[39m \u001b[43mcontrolled_ais_sdeint_ito_em_scan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mg_prod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtfinal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_scheme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_scheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m param_trajectory, ts\n",
      "File \u001b[0;32m/local/scratch/home/fav25/DDS/dds/solvers.py:572\u001b[0m, in \u001b[0;36mcontrolled_ais_sdeint_ito_em_scan\u001b[0;34m(dim, f, b, g, y0, rng, gamma, args, dt, g_prod, step_scheme, start, end, dtype, scheme_args)\u001b[0m\n\u001b[1;32m    568\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out, y_aug\n\u001b[1;32m    570\u001b[0m _, ys_aug \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39mscan(euler_step, (y_pas, t_pas, rng), ts[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys_aug\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), ts\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1791\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(arrays, axis, dtype)\u001b[0m\n\u001b[1;32m   1789\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1791\u001b[0m   arrays_out \u001b[38;5;241m=\u001b[39m [lax\u001b[38;5;241m.\u001b[39mconcatenate(arrays_out[i:i\u001b[38;5;241m+\u001b[39mk], axis)\n\u001b[1;32m   1792\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1791\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1789\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1791\u001b[0m   arrays_out \u001b[38;5;241m=\u001b[39m [\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[38;5;241m0\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dds/lib/python3.10/site-packages/jax/_src/lax/lax.py:3048\u001b[0m, in \u001b[0;36m_concatenate_shape_rule\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m   3046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m({operand\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;28;01mfor\u001b[39;00m operand \u001b[38;5;129;01min\u001b[39;00m operands}) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3047\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot concatenate arrays with different numbers of dimensions: got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3048\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(o\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m operands)))\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m dimension \u001b[38;5;241m<\u001b[39m operands[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m   3050\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenate dimension out of bounds: dimension \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for shapes \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot concatenate arrays with different numbers of dimensions: got (300, 12), (128, 300, 12)."
     ]
    }
   ],
   "source": [
    "# opt and loss setup\n",
    "seq = hk.PRNGSequence(seed)\n",
    "rng_key = next(seq)\n",
    "# subkeys = jax.random.split(rng_key, device_no)\n",
    "subkeys = utils.bcast_local_devices(rng_key)\n",
    "\n",
    "p_init = jax.pmap(\n",
    "    functools.partial(forward_fn.init, batch_size=batch_size_,\n",
    "                        training=True), axis_name=\"num_devices\")\n",
    "\n",
    "params, model_state = p_init(subkeys)\n",
    "\n",
    "trainable_params, non_trainable_params = hk.data_structures.partition(\n",
    "    lambda module, name, value: \"stl_detach\" not in module, params)\n",
    "\n",
    "clipper = optax.clip(1.0)\n",
    "base_dec = config.trainer.lr_sch_base_dec\n",
    "scale_by_adam = optax.scale_by_adam()\n",
    "# if base_dec == 0:\n",
    "#   scale_by_lr = optax.scale(-config.trainer.learning_rate)\n",
    "#   opt = optax.chain(clipper, scale_by_adam, scale_by_lr)\n",
    "# else:\n",
    "transition_steps = 50\n",
    "exp_lr = optax.exponential_decay(config.trainer.learning_rate,\n",
    "                                transition_steps, base_dec)\n",
    "scale_lr = optax.scale_by_schedule(exp_lr)\n",
    "opt = optax.chain(clipper, scale_by_adam, scale_lr, optax.scale(-1))\n",
    "\n",
    "# opt = optax.adam(learning_rate=config.trainer.learning_rate)\n",
    "opt_state = jax.pmap(opt.init)(trainable_params)\n",
    "\n",
    "@functools.partial(\n",
    "    jax.pmap, axis_name=\"num_devices\", static_broadcasted_argnums=(3, 4, 5, 6))\n",
    "def forward_fn_jit(\n",
    "    params,\n",
    "    model_state: hk.State,\n",
    "    subkeys: jnp.ndarray,\n",
    "    batch_size: jnp.ndarray, ode=False, exact=False,  dt_=dt):\n",
    "\n",
    "    samps, _ = forward_fn.apply(\n",
    "        params,\n",
    "        model_state,\n",
    "        subkeys,\n",
    "        int(batch_size / device_no),\n",
    "        False,\n",
    "        ode=ode, exact=exact, dt_=dt_)\n",
    "    samps = jax.device_get(samps)\n",
    "\n",
    "    augmented_trajectory, ts = samps\n",
    "    return (augmented_trajectory, ts), _\n",
    "\n",
    "def forward_fn_wrap(\n",
    "    params,\n",
    "    model_state: hk.State,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: jnp.ndarray, ode=False, exact=False, dt_=dt):\n",
    "    subkeys = jax.random.split(rng_key, device_no)\n",
    "    (augmented_trajectory, ts), _ = forward_fn_jit(params, model_state,\n",
    "                                                subkeys, batch_size, ode, exact,\n",
    "                                                dt_)\n",
    "\n",
    "    dv, ns, t, _ = augmented_trajectory.shape\n",
    "    augmented_trajectory = augmented_trajectory.reshape(dv*ns, t, -1)\n",
    "    return (augmented_trajectory, utils.get_first(ts)), _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_objective(\n",
    "    trainable_params,\n",
    "    non_trainable_params,\n",
    "    model_state: hk.State,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: int,\n",
    "    is_training: bool = True,\n",
    "    ode: bool = False,\n",
    "    stl: bool = False,\n",
    "    exact: bool = False,\n",
    "  ):\n",
    "\n",
    "  params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "  (augmented_trajectory, _), model_state = forward_fn.apply(\n",
    "      params, model_state, rng_key, batch_size, True, ode, exact\n",
    "  )\n",
    "\n",
    "  # import pdb; pdb.set_trace()\n",
    "  gpartial = functools.partial(\n",
    "      config.model.terminal_cost,\n",
    "      lnpi=lnpi, sigma=sigma, tfinal=tfinal, brown=brown)\n",
    "  \n",
    "  if is_training:\n",
    "    loss = config.trainer.objective(\n",
    "        augmented_trajectory, gpartial, source=config.model.source, target=config.model.target, stl=stl, trim=trim, dim=data_dim)\n",
    "  elif not ode:\n",
    "    loss = config.trainer.lnz_is_estimator(\n",
    "        augmented_trajectory, gpartial, source=config.model.source, target=config.model.target, dim=data_dim)\n",
    "  else:\n",
    "    loss = config.trainer.lnz_pf_estimator(\n",
    "        augmented_trajectory, config.model.source, config.model.target)\n",
    "  return loss, model_state\n",
    "\n",
    "@functools.partial(\n",
    "    jax.pmap, axis_name=\"num_devices\", static_broadcasted_argnums=(5,))\n",
    "def update(\n",
    "    trainable_params,\n",
    "    non_trainable_params,\n",
    "    model_state: hk.State,\n",
    "    opt_state: Any,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: jnp.ndarray):\n",
    "  grads, new_model_state = jax.grad(\n",
    "      full_objective, has_aux=True)(\n",
    "          trainable_params,\n",
    "          non_trainable_params,\n",
    "          model_state,\n",
    "          rng_key,\n",
    "          batch_size,\n",
    "          is_training=True,\n",
    "          stl=stl)\n",
    "  grads = jax.lax.pmean(grads, axis_name=\"num_devices\")\n",
    "\n",
    "  updates, opt_state = opt.update(grads, opt_state)\n",
    "  new_params = optax.apply_updates(trainable_params, updates)\n",
    "  return new_params, opt_state, new_model_state\n",
    "\n",
    "@functools.partial(\n",
    "    jax.pmap, axis_name=\"num_devices\", static_broadcasted_argnums=(4, 5, 6, 7))\n",
    "def jited_val_loss(\n",
    "    trainable_params,\n",
    "    non_trainable_params,\n",
    "    model_state: hk.State,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: jnp.ndarray,\n",
    "    is_training: bool = True,\n",
    "    ode: bool = False,\n",
    "    exact: bool = False,):\n",
    "\n",
    "  loss, new_model_state = full_objective(\n",
    "      trainable_params,\n",
    "      non_trainable_params,\n",
    "      model_state,\n",
    "      rng_key,\n",
    "      batch_size,\n",
    "      is_training=is_training, ode=ode,\n",
    "      stl=False, exact=exact,)\n",
    "\n",
    "  loss = jax.lax.pmean(loss, axis_name=\"num_devices\")\n",
    "  return loss, new_model_state\n",
    "\n",
    "def eval_report(\n",
    "    trainable_params,\n",
    "    non_trainable_params,\n",
    "    model_state: hk.State,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: int,\n",
    "    epoch: int,\n",
    "    loss_list: List[float],\n",
    "    is_training: bool = True,\n",
    "    print_flag: bool = False,\n",
    "    ode: bool = False,\n",
    "    exact: bool = False,\n",
    "    wandb_run=None,\n",
    "    wandb_key: Optional[str] = None,\n",
    ") -> None:\n",
    "\n",
    "  loss, model_state = jited_val_loss(\n",
    "      trainable_params, non_trainable_params,\n",
    "      model_state, rng_key, batch_size, is_training, ode, exact)\n",
    "  loss = jax.device_get(loss)\n",
    "  loss = onp.asarray(utils.get_first(loss).item()).item()\n",
    "\n",
    "  log_string = \"epoch: %s %s  loss: %s\", epoch, \"TRAIN\", loss\n",
    "  logging.info(log_string)\n",
    "  if config.trainer.notebook and print_flag: print(log_string)\n",
    "\n",
    "  loss_list.append(loss)\n",
    "  if wandb_run:\n",
    "    wandb_run.log({f\"{wandb_key}/epoch\": epoch, f\"{wandb_key}/loss\": loss})\n",
    "  # writer.flush()\n",
    "\n",
    "loss_list = []\n",
    "loss_list_is = []\n",
    "loss_list_pf = []\n",
    "\n",
    "start = 0\n",
    "times = []\n",
    "\n",
    "run = None\n",
    "\n",
    "for epoch in range(start, config.trainer.epochs):\n",
    "  rng_key = next(seq)\n",
    "  subkeys = jax.random.split(rng_key, device_no)\n",
    "\n",
    "  trainable_params, opt_state, model_state = update(trainable_params,\n",
    "                                                    non_trainable_params,\n",
    "                                                    model_state, opt_state,\n",
    "                                                    subkeys, batch_size_)\n",
    "  if config.trainer.timer:\n",
    "    def func():\n",
    "      return jax.block_until_ready(\n",
    "          update(trainable_params, non_trainable_params, model_state,\n",
    "                opt_state, subkeys, batch_size_))\n",
    "\n",
    "    delta_time = timeit.timeit(func, number=1)\n",
    "    times.append(delta_time)\n",
    "\n",
    "  update_detached_params(trainable_params, non_trainable_params,\n",
    "                        \"simple_drift_net\", \"stl_detach\")\n",
    "\n",
    "  if epoch % config.trainer.log_every_n_epochs == 0:\n",
    "\n",
    "    eval_report(trainable_params, non_trainable_params,\n",
    "                model_state, subkeys, batch_size_elbo, epoch,\n",
    "                loss_list, print_flag=True, wandb_run=run, wandb_key=\"elbo_results\")\n",
    "\n",
    "    eval_report(trainable_params, non_trainable_params,\n",
    "                model_state, subkeys, batch_size_elbo, epoch,\n",
    "                loss_list_is, is_training=False, wandb_run=run, wandb_key=\"is_results\")\n",
    "\n",
    "    eval_report(trainable_params, non_trainable_params,\n",
    "                model_state, subkeys, batch_size_elbo, epoch,\n",
    "                loss_list_pf, is_training=False, ode=True, wandb_run=run, wandb_key=\"pf_results\")\n",
    "\n",
    "    lr = onp.asarray(exp_lr(epoch).item()).item()\n",
    "    if run:\n",
    "        run.log({\"lr/epoch\": epoch, \"lr/lr\": lr})\n",
    "\n",
    "loss_list_is_eval, loss_list_eval, loss_list_pf_eval = [], [], []\n",
    "for i in range(config.eval.seeds):\n",
    "  rng_key = next(seq)\n",
    "  subkeys = jax.random.split(rng_key, device_no)\n",
    "  eval_report(\n",
    "      trainable_params,\n",
    "      non_trainable_params,\n",
    "      model_state,\n",
    "      subkeys,\n",
    "      batch_size_elbo,\n",
    "      i,\n",
    "      loss_list_eval,\n",
    "      print_flag=True,\n",
    "      wandb_run=run,\n",
    "      wandb_key=\"elbo_results_eval\",)\n",
    "\n",
    "  eval_report(\n",
    "      trainable_params,\n",
    "      non_trainable_params,\n",
    "      model_state,\n",
    "      subkeys,\n",
    "      batch_size_elbo,\n",
    "      i,\n",
    "      loss_list_is_eval,\n",
    "      is_training=False,\n",
    "      wandb_run=run,\n",
    "      wandb_key=\"is_results_eval\",)\n",
    "\n",
    "  eval_report(\n",
    "      trainable_params,\n",
    "      non_trainable_params,\n",
    "      model_state,\n",
    "      subkeys,\n",
    "      batch_size_elbo,\n",
    "      i,\n",
    "      loss_list_pf_eval,\n",
    "      is_training=False, ode=True, exact=False,\n",
    "      wandb_run=run,\n",
    "      wandb_key=\"pf_results_eval\",)\n",
    "\n",
    "params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "if config.trainer.timer:\n",
    "  print(times[1:])\n",
    "\n",
    "samps = 2500\n",
    "if method == \"lgcp\" and tfinal >= 12:\n",
    "  samps = 100\n",
    "\n",
    "(augmented_trajectory, _), _ = forward_fn_wrap(params, model_state, rng_key,\n",
    "                                              samps)\n",
    "\n",
    "(augmented_trajectory_det, _), _ = forward_fn_wrap(params, model_state,\n",
    "                                                  rng_key, samps, True, False)\n",
    "  \n",
    "(augmented_trajectory_det_ext, _), _ = forward_fn_wrap(params, model_state,\n",
    "                                                      rng_key, samps, True, True)\n",
    "\n",
    "\n",
    "results_dict = {\n",
    "    \"elbo\": loss_list,\n",
    "    \"is\": loss_list_is,\n",
    "    \"pf\": loss_list_pf,\n",
    "    \"elbo_eval\": loss_list_eval,\n",
    "    \"is_eval\": loss_list_is_eval,\n",
    "    \"pf_eval\": loss_list_pf_eval,\n",
    "    \"aug\": augmented_trajectory,\n",
    "    \"aug_ode\": augmented_trajectory_det,\n",
    "    \"aug_ode_ext\": augmented_trajectory_det_ext\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ode_targ = results_dict[-1][\"aug_ode\"][:, -1,:2]\n",
    "sde_targ = results_dict[-1][\"aug\"][:, -1,:2]\n",
    "\n",
    "plt.plot(ode_targ[:, 0], ode_targ[:, 1], \".\", alpha=0.4)\n",
    "plt.plot(sde_targ[:, 0], sde_targ[:, 1], \".\", alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
