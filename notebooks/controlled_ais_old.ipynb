{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:14:38.195056: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-11 17:14:38.755565: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-11 17:14:38.755654: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-11 17:14:38.755661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dds.configs.config import set_task, get_config\n",
    "from dds.train_dds import train_dds\n",
    "import numpy as onp\n",
    "import jax\n",
    "\n",
    "import distrax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "from jaxline import utils\n",
    "\n",
    "import functools\n",
    "import timeit\n",
    "from typing import Any, List, Tuple, Optional\n",
    "from absl import app, flags\n",
    "\n",
    "from absl import logging\n",
    "import haiku as hk\n",
    "import distrax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from ml_collections import config_dict as configdict\n",
    "from ml_collections import config_flags\n",
    "\n",
    "import numpy as onp\n",
    "import optax\n",
    "\n",
    "from jaxline import utils\n",
    "\n",
    "from dds.configs.config import set_task\n",
    "from dds.data_paths import results_path\n",
    "from dds.utils import flatten_nested_dict\n",
    "import wandb\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dds.objectives import controlled_ais_relative_kl_objective\n",
    "from dds.objectives import controlled_ais_importance_weighted_partition_estimate_dds\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "# Time and step settings (Need to be done before calling set_task)\n",
    "config.model.tfinal = 6.4\n",
    "config.model.dt = 0.05\n",
    "\n",
    "config.model.reference_process_key = \"cais\"\n",
    "\n",
    "config.trainer.objective = controlled_ais_relative_kl_objective\n",
    "config.trainer.lnz_is_estimator = controlled_ais_importance_weighted_partition_estimate_dds\n",
    "\n",
    "if config.model.reference_process_key == \"oudstl\":\n",
    "    config.model.step_scheme_key = \"cos_sq\"\n",
    "\n",
    "config = set_task(config, \"funnel\")\n",
    "config.model.reference_process_key = \"oudstl\"\n",
    "\n",
    "if config.model.reference_process_key == \"oudstl\":\n",
    "    config.model.step_scheme_key = \"cos_sq\"\n",
    "    \n",
    "    # Opt setting for funnel\n",
    "    config.model.sigma = 1.075\n",
    "    config.model.alpha = 0.6875\n",
    "    config.model.m = 1.0\n",
    "        \n",
    "    # Path opt settings    \n",
    "    config.model.exp_dds = False\n",
    "\n",
    "if config.model.reference_process_key == \"cais\":\n",
    "    config.model.step_scheme_key = \"cos_sq\"\n",
    "    \n",
    "    # Opt setting for funnel\n",
    "    config.model.sigma = 1.075\n",
    "    config.model.alpha = 0.6875\n",
    "    config.model.m = 1.0\n",
    "        \n",
    "    # Path opt settings    \n",
    "    config.model.exp_dds = False\n",
    "\n",
    "config.model.stl = False\n",
    "config.model.detach_stl_drift = False\n",
    "\n",
    "config.trainer.notebook = True\n",
    "config.trainer.epochs = 11000\n",
    "# Opt settings we use\n",
    "# funnel_config.trainer.learning_rate = 0.0001\n",
    "config.trainer.learning_rate = 5 * 10**(-3)\n",
    "config.trainer.lr_sch_base_dec = 0.95 # For funnel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_detached_params(trainable_params, non_trainable_params,\n",
    "                           attached_network_name=\"simple_drift_net\",\n",
    "                           detached_network_name=\"stl_detach\"):\n",
    "  \"\"\"Auxiliary function updating detached params for STL.\n",
    "\n",
    "  Args:\n",
    "      trainable_params:\n",
    "      non_trainable_params:\n",
    "      attached_network_name:\n",
    "      detached_network_name:\n",
    "  Returns:\n",
    "    Returns non trainable params\n",
    "  \"\"\"\n",
    "\n",
    "  if len(trainable_params) != len(non_trainable_params):\n",
    "    return non_trainable_params\n",
    "\n",
    "  for key in trainable_params.keys():\n",
    "    if attached_network_name in key:\n",
    "      key_det = key.replace(attached_network_name, detached_network_name)\n",
    "    else:\n",
    "      key_det = key.replace(\"diffusion_network\",\n",
    "                            detached_network_name + \"_diff\")\n",
    "    non_trainable_params[key_det] = trainable_params[key]  # pytype: disable=unsupported-operands\n",
    "\n",
    "  return non_trainable_params\n",
    "\n",
    "data_dim = config.model.input_dim\n",
    "device_no = jax.device_count()\n",
    "\n",
    "results_path = '~/denoising_diffusion_samplers/results'\n",
    "alpha = config.model.alpha\n",
    "sigma = config.model.sigma\n",
    "m = config.model.m\n",
    "\n",
    "# post setup model vars\n",
    "config.model.source_obj = distrax.MultivariateNormalDiag(\n",
    "    jnp.zeros(config.model.input_dim),\n",
    "    config.model.sigma * jnp.ones(config.model.input_dim))\n",
    "config.model.source = config.model.source_obj.log_prob\n",
    "\n",
    "batch_size_ = int(config.model.batch_size / device_no)\n",
    "batch_size_elbo = int(config.model.elbo_batch_size / device_no)\n",
    "\n",
    "step_scheme = config.model.step_scheme_dict[config.model.step_scheme_key]\n",
    "\n",
    "dt = config.model.dt\n",
    "\n",
    "if config.model.reference_process_key == \"oududp\":\n",
    "    key_conversion = {\n",
    "        \"pis\": \"pisudp\",\n",
    "        \"vanilla\": \"vanilla_udp\",\n",
    "        \"tmpis\": \"tmpis_udp\"\n",
    "    }\n",
    "    # \"pisudp\"\n",
    "    config.model.network_key = key_conversion[config.model.network_key]\n",
    "\n",
    "net_key = config.model.network_key\n",
    "network = config.model.network_dict[net_key]\n",
    "\n",
    "tpu = config.model.tpu\n",
    "\n",
    "detach_dif_path, detach_dritf_path = (\n",
    "    config.model.detach_path, config.model.detach_path)\n",
    "\n",
    "target = config.model.target\n",
    "\n",
    "tfinal = config.model.tfinal\n",
    "lnpi = config.trainer.lnpi\n",
    "\n",
    "ref_proc_key = config.model.reference_process_key\n",
    "ref_proc = config.model.reference_process_dict[ref_proc_key]\n",
    "\n",
    "trim = (2 if \"stl\" in str(ref_proc).lower() or \"udp\" in str(ref_proc).lower()\n",
    "        else 1)\n",
    "\n",
    "stl = config.model.stl\n",
    "\n",
    "brown = \"brown\" in str(ref_proc).lower()\n",
    "\n",
    "seed = config.trainer.random_seed  if \"random_seed\" in config.trainer else 42\n",
    "\n",
    "# task directory (currently not in use)\n",
    "task = config.task\n",
    "method = config.model.reference_process_key\n",
    "task_path = results_path + f\"/{task}\" + f\"/{ref_proc_key}\" + f\"/{net_key}\"\n",
    "task_path += f\"/{method}\"\n",
    "\n",
    "\n",
    "# checkpoiting variables for wandb\n",
    "nsteps = config.model.ts.shape[0]\n",
    "keep_every_nth = int(config.trainer.epochs / 125)\n",
    "file_name = (f\"/alpha_{alpha}_sigma_{sigma}_epochs_{config.trainer.epochs}\" +\n",
    "            f\"_task_{task}_seed_{seed}_steps_{nsteps}_stl_{stl}_{method}\" +\n",
    "            f\"_scheme_{config.model.step_scheme_key}_ddpm_test11_chk\")\n",
    "_ = task_path + file_name\n",
    "\n",
    "detach_stl_drift = (\n",
    "    config.model.detach_stl_drift if\n",
    "    \"detach_stl_drift\" in config.model else False\n",
    ")\n",
    "\n",
    "drift_network = lambda: network(config.model, data_dim, \"simple_drift_net\")\n",
    "\n",
    "############## wandb logging  place holder ################\n",
    "data_id = \"denoising_diffusion_samplers\"  # Project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformedWithState(init=<function transform_with_state.<locals>.init_fn at 0x7f7d4c430f70>, apply=<function transform_with_state.<locals>.apply_fn at 0x7f7d4c431240>)\n"
     ]
    }
   ],
   "source": [
    "from turtle import forward\n",
    "\n",
    "\n",
    "def _forward_fn(batch_size: int,\n",
    "                training: bool = True,\n",
    "                ode=False, exact=False, dt_=dt) -> jnp.ndarray:\n",
    "\n",
    "    model_def = ref_proc(\n",
    "        sigma, \n",
    "        data_dim, \n",
    "        drift_network, \n",
    "        tfinal=tfinal, \n",
    "        dt=dt_,\n",
    "        step_scheme=step_scheme, \n",
    "        alpha=alpha, \n",
    "        target=target, \n",
    "        tpu=tpu,\n",
    "        detach_stl_drift=detach_stl_drift, \n",
    "        diff_net=None,\n",
    "        detach_dritf_path=detach_dritf_path, \n",
    "        detach_dif_path=detach_dif_path,\n",
    "        m=m, \n",
    "        log=config.model.log, \n",
    "        exp_bool=config.model.exp_dds, \n",
    "        exact=exact\n",
    "    )\n",
    "\n",
    "    return model_def(batch_size, training, ode=ode)\n",
    "\n",
    "forward_fn = hk.transform_with_state(_forward_fn)  \n",
    "\n",
    "print(forward_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt and loss setup\n",
    "seq = hk.PRNGSequence(seed)\n",
    "rng_key = next(seq)\n",
    "# subkeys = jax.random.split(rng_key, device_no)\n",
    "subkeys = utils.bcast_local_devices(rng_key)\n",
    "\n",
    "p_init = jax.pmap(\n",
    "    functools.partial(forward_fn.init, batch_size=batch_size_,\n",
    "                        training=True), axis_name=\"num_devices\")\n",
    "\n",
    "params, model_state = p_init(subkeys)\n",
    "\n",
    "trainable_params, non_trainable_params = hk.data_structures.partition(\n",
    "    lambda module, name, value: \"stl_detach\" not in module, params)\n",
    "\n",
    "clipper = optax.clip(1.0)\n",
    "base_dec = config.trainer.lr_sch_base_dec\n",
    "scale_by_adam = optax.scale_by_adam()\n",
    "# if base_dec == 0:\n",
    "#   scale_by_lr = optax.scale(-config.trainer.learning_rate)\n",
    "#   opt = optax.chain(clipper, scale_by_adam, scale_by_lr)\n",
    "# else:\n",
    "transition_steps = 50\n",
    "exp_lr = optax.exponential_decay(config.trainer.learning_rate,\n",
    "                                transition_steps, base_dec)\n",
    "scale_lr = optax.scale_by_schedule(exp_lr)\n",
    "opt = optax.chain(clipper, scale_by_adam, scale_lr, optax.scale(-1))\n",
    "\n",
    "# opt = optax.adam(learning_rate=config.trainer.learning_rate)\n",
    "opt_state = jax.pmap(opt.init)(trainable_params)\n",
    "\n",
    "@functools.partial(\n",
    "    jax.pmap, axis_name=\"num_devices\", static_broadcasted_argnums=(3, 4, 5, 6))\n",
    "def forward_fn_jit(\n",
    "    params,\n",
    "    model_state: hk.State,\n",
    "    subkeys: jnp.ndarray,\n",
    "    batch_size: jnp.ndarray, ode=False, exact=False,  dt_=dt):\n",
    "\n",
    "    samps, _ = forward_fn.apply(\n",
    "        params,\n",
    "        model_state,\n",
    "        subkeys,\n",
    "        int(batch_size / device_no),\n",
    "        False,\n",
    "        ode=ode, exact=exact, dt_=dt_)\n",
    "    samps = jax.device_get(samps)\n",
    "\n",
    "    augmented_trajectory, ts = samps\n",
    "    return (augmented_trajectory, ts), _\n",
    "\n",
    "def forward_fn_wrap(\n",
    "    params,\n",
    "    model_state: hk.State,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: jnp.ndarray, ode=False, exact=False, dt_=dt):\n",
    "    subkeys = jax.random.split(rng_key, device_no)\n",
    "    (augmented_trajectory, ts), _ = forward_fn_jit(params, model_state,\n",
    "                                                subkeys, batch_size, ode, exact,\n",
    "                                                dt_)\n",
    "\n",
    "    dv, ns, t, _ = augmented_trajectory.shape\n",
    "    augmented_trajectory = augmented_trajectory.reshape(dv*ns, t, -1)\n",
    "    return (augmented_trajectory, utils.get_first(ts)), _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch: %s %s  loss: %s', 0, 'TRAIN', 17.920591354370117)\n",
      "('epoch: %s %s  loss: %s', 1, 'TRAIN', 16.544363021850586)\n",
      "('epoch: %s %s  loss: %s', 2, 'TRAIN', 15.751117706298828)\n",
      "('epoch: %s %s  loss: %s', 3, 'TRAIN', 14.578880310058594)\n",
      "('epoch: %s %s  loss: %s', 4, 'TRAIN', 12.886594772338867)\n",
      "('epoch: %s %s  loss: %s', 5, 'TRAIN', 9.761495590209961)\n",
      "('epoch: %s %s  loss: %s', 6, 'TRAIN', 3.544013261795044)\n",
      "('epoch: %s %s  loss: %s', 7, 'TRAIN', -3.4844777584075928)\n",
      "('epoch: %s %s  loss: %s', 8, 'TRAIN', -5.274501323699951)\n",
      "('epoch: %s %s  loss: %s', 9, 'TRAIN', -9.568560600280762)\n",
      "('epoch: %s %s  loss: %s', 10, 'TRAIN', -5.726987838745117)\n",
      "('epoch: %s %s  loss: %s', 11, 'TRAIN', -3.0117642879486084)\n",
      "('epoch: %s %s  loss: %s', 12, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 13, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 14, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 15, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 16, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 17, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 18, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 19, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 20, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 21, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 22, 'TRAIN', nan)\n",
      "('epoch: %s %s  loss: %s', 23, 'TRAIN', nan)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m eval_report(trainable_params, non_trainable_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m             model_state, subkeys, batch_size_elbo, epoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m             loss_list, print_flag\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, wandb_run\u001b[39m=\u001b[39mrun, wandb_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39melbo_results\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m eval_report(trainable_params, non_trainable_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m             model_state, subkeys, batch_size_elbo, epoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m             loss_list_is, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, wandb_run\u001b[39m=\u001b[39mrun, wandb_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis_results\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m eval_report(trainable_params, non_trainable_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m             model_state, subkeys, batch_size_elbo, epoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m             loss_list_pf, is_training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, ode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, wandb_run\u001b[39m=\u001b[39;49mrun, wandb_key\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpf_results\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m lr \u001b[39m=\u001b[39m onp\u001b[39m.\u001b[39masarray(exp_lr(epoch)\u001b[39m.\u001b[39mitem())\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39mif\u001b[39;00m run \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb Cell 6\u001b[0m in \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_report\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     trainable_params,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     non_trainable_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m     wandb_key: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m   loss, model_state \u001b[39m=\u001b[39m jited_val_loss(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m       trainable_params, non_trainable_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m       model_state, rng_key, batch_size, is_training, ode, exact)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m   loss \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mdevice_get(loss)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcl/home/sp2058/denoising_diffusion_samplers/notebooks/controlled_ais.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m   loss \u001b[39m=\u001b[39m onp\u001b[39m.\u001b[39masarray(utils\u001b[39m.\u001b[39mget_first(loss)\u001b[39m.\u001b[39mitem())\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def full_objective(\n",
    "    trainable_params,\n",
    "    non_trainable_params,\n",
    "    model_state: hk.State,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: int,\n",
    "    is_training: bool = True,\n",
    "    ode: bool = False,\n",
    "    stl: bool = False,\n",
    "    exact: bool = False,\n",
    "  ):\n",
    "\n",
    "  params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "  (augmented_trajectory, _), model_state = forward_fn.apply(\n",
    "      params, model_state, rng_key, batch_size, True, ode, exact\n",
    "  )\n",
    "\n",
    "  # import pdb; pdb.set_trace()\n",
    "  gpartial = functools.partial(\n",
    "      config.model.terminal_cost,\n",
    "      lnpi=lnpi, sigma=sigma, tfinal=tfinal, brown=brown)\n",
    "  \n",
    "  if is_training:\n",
    "    loss = config.trainer.objective(\n",
    "        augmented_trajectory, gpartial, target=target, stl=stl, trim=trim, dim=data_dim)\n",
    "  elif not ode:\n",
    "    loss = config.trainer.lnz_is_estimator(\n",
    "        augmented_trajectory, gpartial, source=config.model.source, target=config.model.target, dim=data_dim)\n",
    "  else:\n",
    "    loss = config.trainer.lnz_pf_estimator(\n",
    "        augmented_trajectory, config.model.source, config.model.target)\n",
    "  return loss, model_state\n",
    "\n",
    "@functools.partial(\n",
    "    jax.pmap, axis_name=\"num_devices\", static_broadcasted_argnums=(5,))\n",
    "def update(\n",
    "    trainable_params,\n",
    "    non_trainable_params,\n",
    "    model_state: hk.State,\n",
    "    opt_state: Any,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: jnp.ndarray):\n",
    "  grads, new_model_state = jax.grad(\n",
    "      full_objective, has_aux=True)(\n",
    "          trainable_params,\n",
    "          non_trainable_params,\n",
    "          model_state,\n",
    "          rng_key,\n",
    "          batch_size,\n",
    "          is_training=True,\n",
    "          stl=stl)\n",
    "  grads = jax.lax.pmean(grads, axis_name=\"num_devices\")\n",
    "\n",
    "  updates, opt_state = opt.update(grads, opt_state)\n",
    "  new_params = optax.apply_updates(trainable_params, updates)\n",
    "  return new_params, opt_state, new_model_state\n",
    "\n",
    "@functools.partial(\n",
    "    jax.pmap, axis_name=\"num_devices\", static_broadcasted_argnums=(4, 5, 6, 7))\n",
    "def jited_val_loss(\n",
    "    trainable_params,\n",
    "    non_trainable_params,\n",
    "    model_state: hk.State,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: jnp.ndarray,\n",
    "    is_training: bool = True,\n",
    "    ode: bool = False,\n",
    "    exact: bool = False,):\n",
    "\n",
    "  loss, new_model_state = full_objective(\n",
    "      trainable_params,\n",
    "      non_trainable_params,\n",
    "      model_state,\n",
    "      rng_key,\n",
    "      batch_size,\n",
    "      is_training=is_training, ode=ode,\n",
    "      stl=False, exact=exact,)\n",
    "\n",
    "  loss = jax.lax.pmean(loss, axis_name=\"num_devices\")\n",
    "  return loss, new_model_state\n",
    "\n",
    "def eval_report(\n",
    "    trainable_params,\n",
    "    non_trainable_params,\n",
    "    model_state: hk.State,\n",
    "    rng_key: jnp.ndarray,\n",
    "    batch_size: int,\n",
    "    epoch: int,\n",
    "    loss_list: List[float],\n",
    "    is_training: bool = True,\n",
    "    print_flag: bool = False,\n",
    "    ode: bool = False,\n",
    "    exact: bool = False,\n",
    "    wandb_run=None,\n",
    "    wandb_key: Optional[str] = None,\n",
    ") -> None:\n",
    "\n",
    "  loss, model_state = jited_val_loss(\n",
    "      trainable_params, non_trainable_params,\n",
    "      model_state, rng_key, batch_size, is_training, ode, exact)\n",
    "  loss = jax.device_get(loss)\n",
    "  loss = onp.asarray(utils.get_first(loss).item()).item()\n",
    "\n",
    "  log_string = \"epoch: %s %s  loss: %s\", epoch, \"TRAIN\", loss\n",
    "  logging.info(log_string)\n",
    "  if config.trainer.notebook and print_flag: print(log_string)\n",
    "\n",
    "  loss_list.append(loss)\n",
    "  if wandb_run:\n",
    "    wandb_run.log({f\"{wandb_key}/epoch\": epoch, f\"{wandb_key}/loss\": loss})\n",
    "  # writer.flush()\n",
    "\n",
    "loss_list = []\n",
    "loss_list_is = []\n",
    "loss_list_pf = []\n",
    "\n",
    "start = 0\n",
    "times = []\n",
    "\n",
    "run = None\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(start, config.trainer.epochs):\n",
    "  rng_key = next(seq)\n",
    "  subkeys = jax.random.split(rng_key, device_no)\n",
    "\n",
    "  trainable_params, opt_state, model_state = update(trainable_params,\n",
    "                                                    non_trainable_params,\n",
    "                                                    model_state, opt_state,\n",
    "                                                    subkeys, batch_size_)\n",
    "  if config.trainer.timer:\n",
    "    def func():\n",
    "      return jax.block_until_ready(\n",
    "          update(trainable_params, non_trainable_params, model_state,\n",
    "                opt_state, subkeys, batch_size_))\n",
    "\n",
    "    delta_time = timeit.timeit(func, number=1)\n",
    "    times.append(delta_time)\n",
    "\n",
    "  update_detached_params(trainable_params, non_trainable_params,\n",
    "                        \"simple_drift_net\", \"stl_detach\")\n",
    "\n",
    "  if epoch % config.trainer.log_every_n_epochs == 0:\n",
    "\n",
    "    eval_report(trainable_params, non_trainable_params,\n",
    "                model_state, subkeys, batch_size_elbo, epoch,\n",
    "                loss_list, print_flag=True, wandb_run=run, wandb_key=\"elbo_results\")\n",
    "\n",
    "    eval_report(trainable_params, non_trainable_params,\n",
    "                model_state, subkeys, batch_size_elbo, epoch,\n",
    "                loss_list_is, is_training=False, wandb_run=run, wandb_key=\"is_results\")\n",
    "\n",
    "    eval_report(trainable_params, non_trainable_params,\n",
    "                model_state, subkeys, batch_size_elbo, epoch,\n",
    "                loss_list_pf, is_training=False, ode=True, wandb_run=run, wandb_key=\"pf_results\")\n",
    "\n",
    "    lr = onp.asarray(exp_lr(epoch).item()).item()\n",
    "    if run is not None:\n",
    "      run.log({\"lr/epoch\": epoch, \"lr/lr\": lr})\n",
    "\n",
    "loss_list_is_eval, loss_list_eval, loss_list_pf_eval = [], [], []\n",
    "for i in range(config.eval.seeds):\n",
    "  rng_key = next(seq)\n",
    "  subkeys = jax.random.split(rng_key, device_no)\n",
    "  eval_report(\n",
    "      trainable_params,\n",
    "      non_trainable_params,\n",
    "      model_state,\n",
    "      subkeys,\n",
    "      batch_size_elbo,\n",
    "      i,\n",
    "      loss_list_eval,\n",
    "      print_flag=True,\n",
    "      wandb_run=run,\n",
    "      wandb_key=\"elbo_results_eval\",)\n",
    "\n",
    "  eval_report(\n",
    "      trainable_params,\n",
    "      non_trainable_params,\n",
    "      model_state,\n",
    "      subkeys,\n",
    "      batch_size_elbo,\n",
    "      i,\n",
    "      loss_list_is_eval,\n",
    "      is_training=False,\n",
    "      wandb_run=run,\n",
    "      wandb_key=\"is_results_eval\",)\n",
    "\n",
    "  eval_report(\n",
    "      trainable_params,\n",
    "      non_trainable_params,\n",
    "      model_state,\n",
    "      subkeys,\n",
    "      batch_size_elbo,\n",
    "      i,\n",
    "      loss_list_pf_eval,\n",
    "      is_training=False, ode=True, exact=False,\n",
    "      wandb_run=run,\n",
    "      wandb_key=\"pf_results_eval\",)\n",
    "\n",
    "params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "if config.trainer.timer:\n",
    "  print(times[1:])\n",
    "\n",
    "samps = 2500\n",
    "if method == \"lgcp\" and tfinal >= 12:\n",
    "  samps = 100\n",
    "\n",
    "(augmented_trajectory, _), _ = forward_fn_wrap(params, model_state, rng_key,\n",
    "                                              samps)\n",
    "\n",
    "(augmented_trajectory_det, _), _ = forward_fn_wrap(params, model_state,\n",
    "                                                  rng_key, samps, True, False)\n",
    "  \n",
    "(augmented_trajectory_det_ext, _), _ = forward_fn_wrap(params, model_state,\n",
    "                                                      rng_key, samps, True, True)\n",
    "\n",
    "\n",
    "results_dict = {\n",
    "    \"elbo\": loss_list,\n",
    "    \"is\": loss_list_is,\n",
    "    \"pf\": loss_list_pf,\n",
    "    \"elbo_eval\": loss_list_eval,\n",
    "    \"is_eval\": loss_list_is_eval,\n",
    "    \"pf_eval\": loss_list_pf_eval,\n",
    "    \"aug\": augmented_trajectory,\n",
    "    \"aug_ode\": augmented_trajectory_det,\n",
    "    \"aug_ode_ext\": augmented_trajectory_det_ext\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
